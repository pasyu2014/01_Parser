
Парсер может быть полезным инструментом для сбора ссылок с веб-страниц. 
Вот 10 примеров полезных запросов, которые можно реализовать на основе программы, 
и для чего они могут пригодиться:

### 1. Сбор всех внешних ссылок на странице
**Цель**: Получить все ссылки, которые ведут на другие сайты, для анализа внешних ресурсов.
**Реализация**: Использовать функцию `get_links(url)` для получения всех уникальных внешних ссылок.

### 2. Сбор ссылок на определенные типы файлов
**Цель**: Найти ссылки на файлы определенного типа (например, PDF или изображения).
**Реализация**: Модифицировать цикл в `get_links(url)`, чтобы проверять расширение файла (`.pdf`, `.jpg`, `.png` и т.д.) перед добавлением ссылки в множество.

```python
if full_url.endswith(('.pdf', '.jpg', '.png')):
    links.add(full_url)
```

### 3. Проверка доступности внешних ссылок
**Цель**: Проверить, доступны ли найденные внешние ссылки.
**Реализация**: После получения ссылок, использовать `requests.head(link)` для проверки статуса ответа (200, 404 и т.д.).

### 4. Сбор ссылок с нескольких страниц
**Цель**: Получить ссылки с нескольких страниц веб-сайта, например, с пагинации.
**Реализация**: Расширить `parse_links`, чтобы принимать список URL-адресов и повторно использовать `get_links` для каждого.

### 5. Сохранение ссылок в формате JSON
**Цель**: Сохранить собранные ссылки в более структурированном формате для дальнейшего анализа.
**Реализация**: Использовать модуль `json` для записи ссылок в файл формата JSON вместо текстового.

```python
import json
with open(output_file, 'w') as f:
    json.dump(list(visited), f)
```

### 6. Сбор ссылок по ключевым словам
**Цель**: Найти ссылки, содержащие определенные ключевые слова в URL.
**Реализация**: Добавить фильтрацию в `get_links(url)` для проверки наличия ключевых слов в ссылках

```python
if 'keyword' in full_url:
    links.add(full_url)
```

### 7. Определение количества внутренних и внешних ссылок
**Цель**: Получить статистику по количеству внутренних и внешних ссылок на странице.
**Реализация**: В `get_links(url)` использовать два счетчика: один для внутренних, другой для внешних ссылок.

### 8. Анализ ссылок на наличие дублирующихся
**Цель**: Определить, есть ли дублирующиеся ссылки на странице.
**Реализация**: Сравнивать ссылки во время их добавления в множество и сохранять дубликаты в отдельный список.

### 9. Сбор ссылок с учетом глубины
**Цель**: Сбор ссылок с учетом глубины, чтобы избежать перегрузки.
**Реализация**: Использовать аргумент `depth` в `parse_links`, чтобы ограничить уровень обхода.

### 10. Создание отчета о собранных ссылках
**Цель**: Сформировать отчет о собранных ссылках с указанием статистики (например, общее количество, количество уникальных и т.д.).
**Реализация**: После завершения парсинга в `parse_links`, собрать статистику и записать её в файл или выводить в терминал.

```python
print(f"Всего собранных ссылок: {len(visited)}")
```

Эти запросы могут быть полезны в различных задачах, связанных с веб-скрейпингом, SEO-анализом, исследованием конкурентов и многими другими областями, где важно собирать и анализировать ссылки с веб-страниц.